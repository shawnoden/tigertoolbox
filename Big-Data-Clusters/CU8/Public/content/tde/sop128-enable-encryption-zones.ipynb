{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SOP0128 - Enable HDFS Encryption zones in Big Data Clusters.\n",
                "============================================================\n",
                "\n",
                "Description\n",
                "-----------\n",
                "\n",
                "Use this notebook to patch configmaps after Big Data Cluster upgrade to\n",
                "enable HDFS encryption zones for Encryption At Rest.\n",
                "\n",
                "Steps\n",
                "-----\n",
                "\n",
                "### Instantiate Kubernetes client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Instantiate the Python Kubernetes client into 'api' variable\n",
                "\n",
                "import os\n",
                "from IPython.display import Markdown\n",
                "\n",
                "try:\n",
                "    from kubernetes import client, config\n",
                "    from kubernetes.stream import stream\n",
                "\n",
                "    if \"KUBERNETES_SERVICE_PORT\" in os.environ and \"KUBERNETES_SERVICE_HOST\" in os.environ:\n",
                "        config.load_incluster_config()\n",
                "    else:\n",
                "        try:\n",
                "            config.load_kube_config()\n",
                "        except:\n",
                "            display(Markdown(f'HINT: Use [TSG118 - Configure Kubernetes config](../repair/tsg118-configure-kube-config.ipynb) to resolve this issue.'))\n",
                "            raise\n",
                "    api = client.CoreV1Api()\n",
                "\n",
                "    print('Kubernetes client instantiated')\n",
                "except ImportError:\n",
                "    display(Markdown(f'HINT: Use [SOP059 - Install Kubernetes Python module](../install/sop059-install-kubernetes-module.ipynb) to resolve this issue.'))\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get the namespace for the big data cluster\n",
                "\n",
                "Get the namespace of the Big Data Cluster from the Kuberenetes API.\n",
                "\n",
                "**NOTE:**\n",
                "\n",
                "If there is more than one Big Data Cluster in the target Kubernetes\n",
                "cluster, then either:\n",
                "\n",
                "-   set \\[0\\] to the correct value for the big data cluster.\n",
                "-   set the environment variable AZDATA\\_NAMESPACE, before starting\n",
                "    Azure Data Studio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Place Kubernetes namespace name for BDC into 'namespace' variable\n",
                "\n",
                "if \"AZDATA_NAMESPACE\" in os.environ:\n",
                "    namespace = os.environ[\"AZDATA_NAMESPACE\"]\n",
                "else:\n",
                "    try:\n",
                "        namespace = api.list_namespace(label_selector='MSSQL_CLUSTER').items[0].metadata.name\n",
                "    except IndexError:\n",
                "        from IPython.display import Markdown\n",
                "        display(Markdown(f'HINT: Use [TSG081 - Get namespaces (Kubernetes)](../monitor-k8s/tsg081-get-kubernetes-namespaces.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [TSG010 - Get configuration contexts](../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [SOP011 - Set kubernetes configuration context](../common/sop011-set-kubernetes-context.ipynb) to resolve this issue.'))\n",
                "        raise\n",
                "\n",
                "print('The kubernetes namespace for your big data cluster is: ' + namespace)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Common utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import base64\n",
                "import difflib\n",
                "import json\n",
                "import kubernetes.client\n",
                "\n",
                "master_pod_name = 'master-0'\n",
                "hdfs_name_node_pod_name = 'nmnode-0-0'\n",
                "storage_configmap_name = 'mssql-hadoop-storage-0-configmap'\n",
                "\n",
                "def run_command_on_pod(pod, container, command):\n",
                "    return stream(api.connect_get_namespaced_pod_exec, pod, namespace, command=['/bin/sh', '-c', command], container=container, stderr=True, stdout=True)\n",
                "\n",
                "def get_security_json():\n",
                "    # api.connect_get_namespaced_pod_exec returns malformed json with single quotes if output is a valid json\n",
                "    #\n",
                "    return json.loads(base64.b64decode(run_command_on_pod(\n",
                "        pod=master_pod_name, container='mssql-server', command='cat /var/run/configmaps/cluster/security.json | base64')).decode('utf-8'))\n",
                "\n",
                "def is_ad_deployment():\n",
                "    security_json = get_security_json()\n",
                "\n",
                "    if 'environmentVariables' in security_json:\n",
                "        pod_vars = security_json['environmentVariables']\n",
                "        if 'SECURITY_MODE' in pod_vars and pod_vars['SECURITY_MODE'] == 'ActiveDirectory':\n",
                "            print ('The cluster is deployed with Active Directory!')\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def is_bdc_kms_provider_present():\n",
                "    present_message = 'BDC key provider jar exists'\n",
                "    absent_message = 'BDC key provider jar does not exist'\n",
                "    jar_presence = run_command_on_pod(\n",
                "        pod=hdfs_name_node_pod_name, container='hadoop', command=\n",
                "            'ls -la /opt/hadoop/share/hadoop/common/keyprovider/bdc-hadoop-kms-provider-*.jar && ' \\\n",
                "            f'echo \"{present_message}\" || echo \"{absent_message}\"')\n",
                "    return present_message in jar_presence\n",
                "\n",
                "def get_host_fqdn(host):\n",
                "    security_json = get_security_json()\n",
                "\n",
                "    if 'environmentVariables' in security_json:\n",
                "        pod_vars = security_json['environmentVariables']\n",
                "\n",
                "        for domain_var in ['SUBDOMAIN', 'DOMAIN']:\n",
                "            if domain_var in pod_vars and pod_vars[domain_var] != '':\n",
                "                host = f'{host}.{pod_vars[domain_var]}'\n",
                "    return host\n",
                "\n",
                "def configmap_exist(configmap_name):\n",
                "    try:\n",
                "        configmap = api.read_namespaced_config_map(name=configmap_name, namespace=namespace)\n",
                "    except kubernetes.client.rest.ApiException:\n",
                "        return False\n",
                "    return True\n",
                "\n",
                "def read_configmap(configmap_name):\n",
                "    return api.read_namespaced_config_map(name=configmap_name, namespace=namespace)\n",
                "\n",
                "def replace_configmap(configmap_name, patched_configmap):\n",
                "    api.replace_namespaced_config_map(name=configmap_name, namespace=namespace, body=json.loads(patched_configmap)) # works!\n",
                "\n",
                "def can_configmap_patch_be_applied():\n",
                "    if not is_ad_deployment():\n",
                "        return False, 'Warning! Configmap patch can only enable HDFS encryption zone on BDC deployments with Active Directory! Current deployment is non-AD.'\n",
                "    \n",
                "    if not is_bdc_kms_provider_present():\n",
                "        return False, 'Warning! Configmap patch can only enable HDFS encryption zone on BDC deployments with BDC key provider for KMS.'\n",
                "\n",
                "    if not configmap_exist(storage_configmap_name):\n",
                "        return False, f'Warning! Configmap patch cannot be applied because BDC storage configmap {storage_configmap_name} does not exist.'\n",
                "    \n",
                "    return True, 'Ok'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Patching configmaps `mssql-hadoop-storage-0-configmap`, `mssql-hadoop-sparkhead-configmap`, `mssql-hadoop-spark-0-configmap` after BDC upgrade from earlier versions to enable HDFS encryption zones support in Big Data Cluster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import xml.etree.ElementTree as ET\n",
                "\n",
                "# Preserving existing comments in XML document\n",
                "#\n",
                "class CommentedTreeBuilder(ET.TreeBuilder):\n",
                "    def comment(self, data):\n",
                "        self.start(ET.Comment, {})\n",
                "        self.data(data)\n",
                "        self.end(ET.Comment)\n",
                "\n",
                "def patch_configmap_json(configmap_json):\n",
                "    core_site = 'core-site.xml'\n",
                "    cs = configmap_json['data'][core_site]\n",
                "\n",
                "    parser = ET.XMLParser(target=CommentedTreeBuilder())\n",
                "    csxml = ET.fromstring(cs, parser=parser)\n",
                "    props = [prop.find('value') for prop in csxml.findall(\n",
                "        \"./property/name/..[name='hadoop.security.key.provider.path']\")]\n",
                "\n",
                "    if len(props) == 0:\n",
                "        prop = ET.SubElement(csxml, 'property')\n",
                "        name = ET.SubElement(prop, 'name')\n",
                "        name.text = 'hadoop.security.key.provider.path'\n",
                "        value = ET.SubElement(prop, 'value')\n",
                "        key_provider_path_tag = value\n",
                "    else:\n",
                "        key_provider_path_tag = props[0]\n",
                "\n",
                "    # Get HDFS Name Node FQDN repeated 5 times so that HDFS clients will\n",
                "    # retry when single name Name Node is unavailable.\n",
                "    #\n",
                "    repeat_fqdn_n_times = 5\n",
                "    name_node_fqdn = get_host_fqdn(hdfs_name_node_pod_name)\n",
                "    name_node_fqdn = ';'.join([name_node_fqdn] * repeat_fqdn_n_times)\n",
                "\n",
                "    key_provider_path_tag.text = f'kms://https@{name_node_fqdn}:9600/kms'\n",
                "    cs = ET.tostring(csxml, encoding='utf-8').decode('utf-8')\n",
                "    configmap_json['data'][core_site] = cs\n",
                "\n",
                "    kms_site = 'kms-site.xml.tmpl'\n",
                "    ks = configmap_json['data'][kms_site]\n",
                "\n",
                "    kp_uri_regex = re.compile('(<name>hadoop.kms.key.provider.uri</name>\\s*<value>\\s*)(.*)(\\s*</value>)', re.MULTILINE)\n",
                "\n",
                "    hdfsvault_svc_fqdn = get_host_fqdn('hdfsvault-svc')\n",
                "\n",
                "    def replace_uri(match_obj):\n",
                "        if match_obj.group(2) == 'jceks://file@/var/run/secrets/keystores/kms/kms.jceks':\n",
                "            return match_obj.group(1) + f'bdc://https@{hdfsvault_svc_fqdn}' + match_obj.group(3)\n",
                "        return match_obj.group(0)\n",
                "\n",
                "    ks = kp_uri_regex.sub(replace_uri, ks)\n",
                "\n",
                "    configmap_json['data'][kms_site] = ks\n",
                "    return json.dumps(configmap_json, indent=4, sort_keys=True)\n",
                "\n",
                "def patch_configmap_if_needed(configmap_name):\n",
                "    configmap = read_configmap(configmap_name)\n",
                "\n",
                "    # Fix for model objects not valid for serialization\n",
                "    #\n",
                "    configmap = kubernetes.client.ApiClient().sanitize_for_serialization(configmap)\n",
                "    pretty_configmap = json.dumps(configmap, indent=4, sort_keys=True)\n",
                "\n",
                "    patched_configmap = patch_configmap_json(configmap)\n",
                "\n",
                "    diff_lines = [line for line in difflib.context_diff(pretty_configmap.split(), patched_configmap.split(), fromfile='before patch', tofile='after patch')]\n",
                "    if len(diff_lines) > 0:\n",
                "        print(f'Diff between configmap {configmap_name} before and after patch:')\n",
                "        for line in diff_lines:\n",
                "            print(line)\n",
                "\n",
                "        replace_configmap(configmap_name, patched_configmap)\n",
                "        print(f'Applying patch for configmap {configmap_name} to enable support of HDFS encryption zones with BDC key provider.')\n",
                "    else:\n",
                "        print(f'Configmap {configmap_name} is already patched to enable support of HDFS encryption zones with BDC key provider.')\n",
                "\n",
                "can_be_applied, reason = can_configmap_patch_be_applied()\n",
                "if can_be_applied:\n",
                "    for configmap_name in [storage_configmap_name, 'mssql-hadoop-sparkhead-configmap', 'mssql-hadoop-spark-0-configmap']:\n",
                "        if configmap_exist(configmap_name):\n",
                "            patch_configmap_if_needed(configmap_name)\n",
                "        else:\n",
                "            print(f'Skipping configmap {configmap_name} because it does not exist on this BDC deployment.')\n",
                "else:\n",
                "    print(reason)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Restart pods to apply changes of configmaps\n",
                "\n",
                "### Common functions\n",
                "\n",
                "Define helper functions used in this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "# Define `run` function for transient fault handling, suggestions on error, and scrolling updates on Windows\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "import json\n",
                "import platform\n",
                "import shlex\n",
                "import shutil\n",
                "import datetime\n",
                "\n",
                "from subprocess import Popen, PIPE\n",
                "from IPython.display import Markdown\n",
                "\n",
                "retry_hints = {} # Output in stderr known to be transient, therefore automatically retry\n",
                "error_hints = {} # Output in stderr where a known SOP/TSG exists which will be HINTed for further help\n",
                "install_hint = {} # The SOP to help install the executable if it cannot be found\n",
                "\n",
                "first_run = True\n",
                "rules = None\n",
                "debug_logging = False\n",
                "\n",
                "def run(cmd, return_output=False, no_output=False, retry_count=0, base64_decode=False, return_as_json=False):\n",
                "    \"\"\"Run shell command, stream stdout, print stderr and optionally return output\n",
                "\n",
                "    NOTES:\n",
                "\n",
                "    1.  Commands that need this kind of ' quoting on Windows e.g.:\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='data-pool')].metadata.name}\n",
                "\n",
                "        Need to actually pass in as '\"':\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='\"'data-pool'\"')].metadata.name}\n",
                "\n",
                "        The ' quote approach, although correct when pasting into Windows cmd, will hang at the line:\n",
                "        \n",
                "            `iter(p.stdout.readline, b'')`\n",
                "\n",
                "        The shlex.split call does the right thing for each platform, just use the '\"' pattern for a '\n",
                "    \"\"\"\n",
                "    MAX_RETRIES = 5\n",
                "    output = \"\"\n",
                "    retry = False\n",
                "\n",
                "    global first_run\n",
                "    global rules\n",
                "\n",
                "    if first_run:\n",
                "        first_run = False\n",
                "        rules = load_rules()\n",
                "\n",
                "    # When running `azdata sql query` on Windows, replace any \\n in \"\"\" strings, with \" \", otherwise we see:\n",
                "    #\n",
                "    #    ('HY090', '[HY090] [Microsoft][ODBC Driver Manager] Invalid string or buffer length (0) (SQLExecDirectW)')\n",
                "    #\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"azdata sql query\"):\n",
                "        cmd = cmd.replace(\"\\n\", \" \")\n",
                "\n",
                "    # shlex.split is required on bash and for Windows paths with spaces\n",
                "    #\n",
                "    cmd_actual = shlex.split(cmd)\n",
                "\n",
                "    # Store this (i.e. kubectl, python etc.) to support binary context aware error_hints and retries\n",
                "    #\n",
                "    user_provided_exe_name = cmd_actual[0].lower()\n",
                "\n",
                "    # When running python, use the python in the ADS sandbox ({sys.executable})\n",
                "    #\n",
                "    if cmd.startswith(\"python \"):\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"python\", sys.executable)\n",
                "\n",
                "        # On Mac, when ADS is not launched from terminal, LC_ALL may not be set, which causes pip installs to fail\n",
                "        # with:\n",
                "        #\n",
                "        #    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4969: ordinal not in range(128)\n",
                "        #\n",
                "        # Setting it to a default value of \"en_US.UTF-8\" enables pip install to complete\n",
                "        #\n",
                "        if platform.system() == \"Darwin\" and \"LC_ALL\" not in os.environ:\n",
                "            os.environ[\"LC_ALL\"] = \"en_US.UTF-8\"\n",
                "\n",
                "    # When running `kubectl`, if AZDATA_OPENSHIFT is set, use `oc`\n",
                "    #\n",
                "    if cmd.startswith(\"kubectl \") and \"AZDATA_OPENSHIFT\" in os.environ:\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"kubectl\", \"oc\")\n",
                "\n",
                "    # To aid supportability, determine which binary file will actually be executed on the machine\n",
                "    #\n",
                "    which_binary = None\n",
                "\n",
                "    # Special case for CURL on Windows.  The version of CURL in Windows System32 does not work to\n",
                "    # get JWT tokens, it returns \"(56) Failure when receiving data from the peer\".  If another instance\n",
                "    # of CURL exists on the machine use that one.  (Unfortunately the curl.exe in System32 is almost\n",
                "    # always the first curl.exe in the path, and it can't be uninstalled from System32, so here we\n",
                "    # look for the 2nd installation of CURL in the path)\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"curl \"):\n",
                "        path = os.getenv('PATH')\n",
                "        for p in path.split(os.path.pathsep):\n",
                "            p = os.path.join(p, \"curl.exe\")\n",
                "            if os.path.exists(p) and os.access(p, os.X_OK):\n",
                "                if p.lower().find(\"system32\") == -1:\n",
                "                    cmd_actual[0] = p\n",
                "                    which_binary = p\n",
                "                    break\n",
                "\n",
                "    # Find the path based location (shutil.which) of the executable that will be run (and display it to aid supportability), this\n",
                "    # seems to be required for .msi installs of azdata.cmd/az.cmd.  (otherwise Popen returns FileNotFound) \n",
                "    #\n",
                "    # NOTE: Bash needs cmd to be the list of the space separated values hence shlex.split.\n",
                "    #\n",
                "    if which_binary == None:\n",
                "        which_binary = shutil.which(cmd_actual[0])\n",
                "\n",
                "    # Display an install HINT, so the user can click on a SOP to install the missing binary\n",
                "    #\n",
                "    if which_binary == None:\n",
                "        print(f\"The path used to search for '{cmd_actual[0]}' was:\")\n",
                "        print(sys.path)\n",
                "\n",
                "        if user_provided_exe_name in install_hint and install_hint[user_provided_exe_name] is not None:\n",
                "            display(Markdown(f'HINT: Use [{install_hint[user_provided_exe_name][0]}]({install_hint[user_provided_exe_name][1]}) to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\")\n",
                "    else:   \n",
                "        cmd_actual[0] = which_binary\n",
                "\n",
                "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
                "\n",
                "    print(f\"START: {cmd} @ {start_time} ({datetime.datetime.utcnow().replace(microsecond=0)} UTC)\")\n",
                "    print(f\"       using: {which_binary} ({platform.system()} {platform.release()} on {platform.machine()})\")\n",
                "    print(f\"       cwd: {os.getcwd()}\")\n",
                "\n",
                "    # Command-line tools such as CURL and AZDATA HDFS commands output\n",
                "    # scrolling progress bars, which causes Jupyter to hang forever, to\n",
                "    # workaround this, use no_output=True\n",
                "    #\n",
                "\n",
                "\n",
                "    # Work around a infinite hang when a notebook generates a non-zero return code, break out, and do not wait\n",
                "    #\n",
                "    wait = True \n",
                "\n",
                "    try:\n",
                "        if no_output:\n",
                "            p = Popen(cmd_actual)\n",
                "        else:\n",
                "            p = Popen(cmd_actual, stdout=PIPE, stderr=PIPE, bufsize=1)\n",
                "            with p.stdout:\n",
                "                for line in iter(p.stdout.readline, b''):\n",
                "                    line = line.decode()\n",
                "                    if return_output:\n",
                "                        output = output + line\n",
                "                    else:\n",
                "                        if cmd.startswith(\"azdata notebook run\"): # Hyperlink the .ipynb file\n",
                "                            regex = re.compile('  \"(.*)\"\\: \"(.*)\"') \n",
                "                            match = regex.match(line)\n",
                "                            if match:\n",
                "                                if match.group(1).find(\"HTML\") != -1:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"{match.group(2)}\"'))\n",
                "                                else:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"[{match.group(2)}]({match.group(2)})\"'))\n",
                "\n",
                "                                    wait = False\n",
                "                                    break # otherwise infinite hang, have not worked out why yet.\n",
                "                        else:\n",
                "                            print(line, end='')\n",
                "                            if rules is not None:\n",
                "                                apply_expert_rules(line)\n",
                "\n",
                "        if wait:\n",
                "            p.wait()\n",
                "    except FileNotFoundError as e:\n",
                "        if install_hint is not None:\n",
                "            display(Markdown(f'HINT: Use {install_hint} to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\") from e\n",
                "\n",
                "    exit_code_workaround = 0 # WORKAROUND: azdata hangs on exception from notebook on p.wait()\n",
                "\n",
                "    if not no_output:\n",
                "        for line in iter(p.stderr.readline, b''):\n",
                "            try:\n",
                "                line_decoded = line.decode()\n",
                "            except UnicodeDecodeError:\n",
                "                # NOTE: Sometimes we get characters back that cannot be decoded(), e.g.\n",
                "                #\n",
                "                #   \\xa0\n",
                "                #\n",
                "                # For example see this in the response from `az group create`:\n",
                "                #\n",
                "                # ERROR: Get Token request returned http error: 400 and server \n",
                "                # response: {\"error\":\"invalid_grant\",# \"error_description\":\"AADSTS700082: \n",
                "                # The refresh token has expired due to inactivity.\\xa0The token was \n",
                "                # issued on 2018-10-25T23:35:11.9832872Z\n",
                "                #\n",
                "                # which generates the exception:\n",
                "                #\n",
                "                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 179: invalid start byte\n",
                "                #\n",
                "                print(\"WARNING: Unable to decode stderr line, printing raw bytes:\")\n",
                "                print(line)\n",
                "                line_decoded = \"\"\n",
                "                pass\n",
                "            else:\n",
                "\n",
                "                # azdata emits a single empty line to stderr when doing an hdfs cp, don't\n",
                "                # print this empty \"ERR:\" as it confuses.\n",
                "                #\n",
                "                if line_decoded == \"\":\n",
                "                    continue\n",
                "                \n",
                "                print(f\"STDERR: {line_decoded}\", end='')\n",
                "\n",
                "                if line_decoded.startswith(\"An exception has occurred\") or line_decoded.startswith(\"ERROR: An error occurred while executing the following cell\"):\n",
                "                    exit_code_workaround = 1\n",
                "\n",
                "                # inject HINTs to next TSG/SOP based on output in stderr\n",
                "                #\n",
                "                if user_provided_exe_name in error_hints:\n",
                "                    for error_hint in error_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(error_hint[0]) != -1:\n",
                "                            display(Markdown(f'HINT: Use [{error_hint[1]}]({error_hint[2]}) to resolve this issue.'))\n",
                "\n",
                "                # apply expert rules (to run follow-on notebooks), based on output\n",
                "                #\n",
                "                if rules is not None:\n",
                "                    apply_expert_rules(line_decoded)\n",
                "\n",
                "                # Verify if a transient error, if so automatically retry (recursive)\n",
                "                #\n",
                "                if user_provided_exe_name in retry_hints:\n",
                "                    for retry_hint in retry_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(retry_hint) != -1:\n",
                "                            if retry_count < MAX_RETRIES:\n",
                "                                print(f\"RETRY: {retry_count} (due to: {retry_hint})\")\n",
                "                                retry_count = retry_count + 1\n",
                "                                output = run(cmd, return_output=return_output, retry_count=retry_count)\n",
                "\n",
                "                                if return_output:\n",
                "                                    if base64_decode:\n",
                "                                        import base64\n",
                "\n",
                "                                        return base64.b64decode(output).decode('utf-8')\n",
                "                                    else:\n",
                "                                        return output\n",
                "\n",
                "    elapsed = datetime.datetime.now().replace(microsecond=0) - start_time\n",
                "\n",
                "    # WORKAROUND: We avoid infinite hang above in the `azdata notebook run` failure case, by inferring success (from stdout output), so\n",
                "    # don't wait here, if success known above\n",
                "    #\n",
                "    if wait: \n",
                "        if p.returncode != 0:\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(p.returncode)}.\\n')\n",
                "    else:\n",
                "        if exit_code_workaround !=0 :\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(exit_code_workaround)}.\\n')\n",
                "\n",
                "    print(f'\\nSUCCESS: {elapsed}s elapsed.\\n')\n",
                "\n",
                "    if return_output:\n",
                "        if base64_decode:\n",
                "            import base64\n",
                "\n",
                "            return base64.b64decode(output).decode('utf-8')\n",
                "        else:\n",
                "            return output\n",
                "\n",
                "def load_json(filename):\n",
                "    \"\"\"Load a json file from disk and return the contents\"\"\"\n",
                "\n",
                "    with open(filename, encoding=\"utf8\") as json_file:\n",
                "        return json.load(json_file)\n",
                "\n",
                "def load_rules():\n",
                "    \"\"\"Load any 'expert rules' from the metadata of this notebook (.ipynb) that should be applied to the stderr of the running executable\"\"\"\n",
                "\n",
                "    # Load this notebook as json to get access to the expert rules in the notebook metadata.\n",
                "    #\n",
                "    try:\n",
                "        j = load_json(\"sop128-enable-encryption-zones.ipynb\")\n",
                "    except:\n",
                "        pass # If the user has renamed the book, we can't load ourself.  NOTE: Is there a way in Jupyter, to know your own filename?\n",
                "    else:\n",
                "        if \"metadata\" in j and \\\n",
                "            \"azdata\" in j[\"metadata\"] and \\\n",
                "            \"expert\" in j[\"metadata\"][\"azdata\"] and \\\n",
                "            \"expanded_rules\" in j[\"metadata\"][\"azdata\"][\"expert\"]:\n",
                "\n",
                "            rules = j[\"metadata\"][\"azdata\"][\"expert\"][\"expanded_rules\"]\n",
                "\n",
                "            rules.sort() # Sort rules, so they run in priority order (the [0] element).  Lowest value first.\n",
                "\n",
                "            # print (f\"EXPERT: There are {len(rules)} rules to evaluate.\")\n",
                "\n",
                "            return rules\n",
                "\n",
                "def apply_expert_rules(line):\n",
                "    \"\"\"Determine if the stderr line passed in, matches the regular expressions for any of the 'expert rules', if so\n",
                "    inject a 'HINT' to the follow-on SOP/TSG to run\"\"\"\n",
                "\n",
                "    global rules\n",
                "\n",
                "    for rule in rules:\n",
                "        notebook = rule[1]\n",
                "        cell_type = rule[2]\n",
                "        output_type = rule[3] # i.e. stream or error\n",
                "        output_type_name = rule[4] # i.e. ename or name \n",
                "        output_type_value = rule[5] # i.e. SystemExit or stdout\n",
                "        details_name = rule[6]  # i.e. evalue or text \n",
                "        expression = rule[7].replace(\"\\\\*\", \"*\") # Something escaped *, and put a \\ in front of it!\n",
                "\n",
                "        if debug_logging:\n",
                "            print(f\"EXPERT: If rule '{expression}' satisfied', run '{notebook}'.\")\n",
                "\n",
                "        if re.match(expression, line, re.DOTALL):\n",
                "\n",
                "            if debug_logging:\n",
                "                print(\"EXPERT: MATCH: name = value: '{0}' = '{1}' matched expression '{2}', therefore HINT '{4}'\".format(output_type_name, output_type_value, expression, notebook))\n",
                "\n",
                "            match_found = True\n",
                "\n",
                "            display(Markdown(f'HINT: Use [{notebook}]({notebook}) to resolve this issue.'))\n",
                "\n",
                "\n",
                "\n",
                "print('Common functions defined successfully.')\n",
                "\n",
                "# Hints for binary (transient fault) retry, (known) error and install guide\n",
                "#\n",
                "retry_hints = {'kubectl': ['A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond']}\n",
                "error_hints = {'kubectl': [['no such host', 'TSG010 - Get configuration contexts', '../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb'], ['No connection could be made because the target machine actively refused it', 'TSG056 - Kubectl fails with No connection could be made because the target machine actively refused it', '../repair/tsg056-kubectl-no-connection-could-be-made.ipynb']]}\n",
                "install_hint = {'kubectl': ['SOP036 - Install kubectl command line interface', '../install/sop036-install-kubectl.ipynb']}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameters for waiting for healthy cluster state"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "timeout = 600  # amount of time in seconds to wait before cluster is healthy:  default to 10 minutes\n",
                "check_interval = 30  # amount of time in seconds between health checks - default 30 seconds\n",
                "min_pod_count = 10  # minimum number of healthy pods required to assert health"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper functions for waiting for the cluster to become healthy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "source": [
                "import threading\n",
                "import time\n",
                "import sys\n",
                "import os\n",
                "from IPython.display import Markdown\n",
                "\n",
                "isRunning = True\n",
                "\n",
                "def all_containers_ready(pod):\n",
                "    \"\"\"helper method returns true if all the containers within the given pod are ready\n",
                "\n",
                "    Arguments:\n",
                "        pod {v1Pod} -- Metadata retrieved from the api call to.\n",
                "    \"\"\"\n",
                "         \n",
                "    return all(map(lambda c: c.ready is True, pod.status.container_statuses))\n",
                "\n",
                "\n",
                "def pod_is_ready(pod):\n",
                "    \"\"\"tests that the pod, and all containers are ready\n",
                "\n",
                "    Arguments:\n",
                "        pod {v1Pod} -- Metadata retrieved from api call.\n",
                "    \"\"\"\n",
                "\n",
                "    return \"job-name\" in pod.metadata.labels or (pod.status.phase == \"Running\" and all_containers_ready(pod))\n",
                "\n",
                "\n",
                "def waitReady():\n",
                "    \"\"\"Waits for all pods, and containers to become ready.\n",
                "    \"\"\"\n",
                "    while isRunning:\n",
                "        try:\n",
                "            time.sleep(check_interval)\n",
                "            pods = get_pods()\n",
                "            allReady = len(pods.items) >= min_pod_count and all(map(pod_is_ready, pods.items))\n",
                "\n",
                "            if allReady:\n",
                "                return True\n",
                "            else:\n",
                "                display(Markdown(get_pod_failures(pods)))\n",
                "                display(Markdown(f\"cluster not healthy, rechecking in {check_interval} seconds.\"))\n",
                "        except Exception as ex:\n",
                "            last_error_message = str(ex)\n",
                "            display(Markdown(last_error_message))\n",
                "            time.sleep(check_interval)\n",
                "\n",
                "def get_pod_failures(pods=None):\n",
                "    \"\"\"Returns a status message for any pods that are not ready.\n",
                "    \"\"\"\n",
                "    results = \"\"\n",
                "    if not pods:\n",
                "        pods = get_pods()\n",
                "\n",
                "    for pod in pods.items:\n",
                "        if \"job-name\" not in pod.metadata.labels:\n",
                "            if pod.status and pod.status.container_statuses:\n",
                "                for container in filter(lambda c: c.ready is False, pod.status.container_statuses):\n",
                "                    results = results + \"Container {0} in Pod {1} is not ready. Reported status: {2} <br/>\".format(container.name, pod.metadata.name, container.state)       \n",
                "            else:\n",
                "                results = results + \"Pod {0} is not ready.  <br/>\".format(pod.metadata.name)\n",
                "    return results\n",
                "\n",
                "\n",
                "def get_pods():\n",
                "    \"\"\"Returns a list of pods by namespace, or all namespaces if no namespace is specified\n",
                "    \"\"\"\n",
                "    pods = None\n",
                "    if namespace is not None:\n",
                "        display(Markdown(f'Checking namespace {namespace}'))\n",
                "        pods = api.list_namespaced_pod(namespace, _request_timeout=30) \n",
                "    else:\n",
                "        display(Markdown('Checking all namespaces'))\n",
                "        pods = api.list_pod_for_all_namespaces(_request_timeout=30)\n",
                "    return pods\n",
                "\n",
                "def wait_for_cluster_healthy():\n",
                "    isRunning = True\n",
                "    mt = threading.Thread(target=waitReady)\n",
                "    mt.start()\n",
                "    mt.join(timeout=timeout)\n",
                "\n",
                "    if mt.isAlive():\n",
                "      raise SystemExit(\"Timeout waiting for all cluster to be healthy.\")\n",
                "      \n",
                "    isRunning = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "def restart_pods_if_needed():\n",
                "    # Prevent unnecessary pod restart on deployments where HDFS encryption zones cannot be enabled.\n",
                "    #\n",
                "    can_be_applied, reason = can_configmap_patch_be_applied()\n",
                "    if not can_be_applied:\n",
                "        print(reason)\n",
                "        return\n",
                "\n",
                "    pods_list_string = run(f'kubectl get pod -n {namespace} -o=name', return_output=True)\n",
                "\n",
                "    # Remember count of healthy pods\n",
                "    #\n",
                "    min_pod_count = len(get_pods().items)\n",
                "\n",
                "    pod_regex = re.compile('pod/(?P<pod>(nmnode-0|storage-0|sparkhead|spark-0)-[0-9]+)\\s*')\n",
                "    pods = (pod_match.group('pod') for pod_match in pod_regex.finditer(pods_list_string))\n",
                "    for pod in pods:\n",
                "        print(f'Restarting pod {pod}')\n",
                "        run(f'kubectl delete pod {pod} -n {namespace}')\n",
                "\n",
                "        # Do not restart more than one pod at once\n",
                "        #\n",
                "        wait_for_cluster_healthy()\n",
                "\n",
                "    print(\"Restarted pods. HDFS Encryption zones can now be used in the cluster.\")\n",
                "    \n",
                "restart_pods_if_needed()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Notebook execution complete.')"
            ]
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "azdata": {
            "side_effects": true
        }
    }
}