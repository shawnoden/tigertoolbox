{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "pansop": {
            "related": "",
            "test": {
                "strategy": "",
                "types": null,
                "disable": {
                    "reason": "",
                    "workitems": null,
                    "types": null
                }
            },
            "target": {
                "current": "",
                "final": ""
            },
            "internal": {
                "parameters": null,
                "symlink": false
            },
            "timeout": "0"
        },
        "widgets": [],
        "extensions": {
            "azuredatastudio": {
                "version": 1,
                "views": []
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "SOP008 - Backup HDFS files to Azure Data Lake Store Gen2 with distcp\n",
                "====================================================================\n",
                "\n",
                "Description\n",
                "-----------\n",
                "\n",
                "This Standard Operating Procedure (SOP) backs up data from the source\n",
                "SQL Server 2019 BDC cluster’s HDFS filesystem to the Azure Data Lake\n",
                "Store Gen2 account you specify. Please ensure the Azure Data Lake Store\n",
                "Gen2 account is configured with “hierarchical namespace” enabled.\n",
                "\n",
                "**Why should I backup my HDFS data?**\n",
                "\n",
                "HDFS is a distributed block-oriented file-store. Each file stored in the\n",
                "hdfs filesystem is comprised of one or more blocks. Redundant copies of\n",
                "each block are automatically maintained by HDFS depending on the\n",
                "“replication factor” for a given file. The default replication factor\n",
                "for the cluster, fs.replication, is defined in the hdfs-site.xml\n",
                "configuration file.\n",
                "\n",
                "Even though HDFS stores redundant copies of each block, files can still\n",
                "become corrupt or lost due to accidental user actions, code bugs and\n",
                "hardware failures. You can use the hadoop distcp command to backup HDFS\n",
                "files that are business critical, and cannot be easily re-created in the\n",
                "event of their loss.\n",
                "\n",
                "**Azure Data Lake Storage Gen2 Required Permissions**\n",
                "\n",
                "This notebook uses the primary storage key of the Azure Data Lake Store\n",
                "Gen2 storage account to authorize storage account access. In order to\n",
                "successfully run this script, the principal must possess permission to\n",
                "list the keys of the Data Lake Store Gen2 account specified as the\n",
                "“target” of the distcp operation.\n",
                "\n",
                "You can read more about Azure Storage Access Control\n",
                "[here](https://docs.microsoft.com/en-us/azure/storage/common/storage-security-guide?toc=%2fazure%2fstorage%2fblobs%2ftoc.json#granting-access).\n",
                "\n",
                "**Notebook Dependencies**\n",
                "\n",
                "This script has a dependency on [Azure\n",
                "CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest),\n",
                "and Azure CLI must be installed on the machine where you are running\n",
                "Azure Data Studio.\n",
                "\n",
                "If your SQL Server Big Data Cluster is deployed on Azure Kubernetes\n",
                "Service, Azure CLI is a [required\n",
                "tool](https://docs.microsoft.com/en-us/sql/big-data-cluster/deploy-big-data-tools?view=sql-server-ver15)\n",
                "and may already be installed.\n",
                "\n",
                "Steps\n",
                "-----\n",
                "\n",
                "### Parameters\n",
                "\n",
                "In the cell below, specify the source-directory of the files in the SQL\n",
                "Server 2019 BDC cluster’s HDFS filesystem to backup, and the Azure Data\n",
                "Lake Storage Gen2 storage account where the backup data will be written."
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "9f96f03e-eb66-4f20-aa6e-d2d557eb8ca2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# The azure subscription ID for the Azure Data Lake Store Gen2 account.  If not specified, the default subscription for the logged in user will be used\n",
                "azure_subscription_id = \"<insert azure subscription id>\"\n",
                "\n",
                "# The HDFS directory that distcp will backup to the ADL Gen2 account specified on the adl_gen2_storage_account_name parameter (see below).  \n",
                "# If there was a directory named \"data/flight-data\" located in the root of the HDFS filesystem, it would be specified as follows:\n",
                "# bdc_hdfs_source_directory='/data/flight-data'\n",
                "#\n",
                "bdc_hdfs_source_directory='/<directory-to-backup>'\n",
                "\n",
                "\n",
                "# Three ADL Gen2 parameters are required to specify the location in the ADL Gen2 filesystem to backup the hdfs_source_directory to; numbered 1, 2 and 3 below.\n",
                "# They are used to construct the final URI used by distcp as follows\n",
                "# abfss://<adl_gen2_filesystem>@<adl_gen2_storage_account_name>.dfs.core.windows.net/<adl_gen2_backup_destination_folder>\n",
                "\n",
                "# 1. The name of the ADL Gen2 storage account the BDC cluster's HDFS data will be backed up to\n",
                "#\n",
                "adl_gen2_storage_account_name='<your-Azure-data-lake-store-gen2-account-name>'\n",
                "\n",
                "# 2. The name of the \"target\" Azure Data Lake Store Gen2 filesystem to backup to\n",
                "#\n",
                "adl_gen2_filesystem='<adl-gen2-filesystem-name>'\n",
                "\n",
                "# 3. The \"target\" directory to backup to within the adl_gen2_filesystem specified as a full path:\n",
                "# adl_gen2_backup_destination_folder='/backup/clusters/bdc'\n",
                "#\n",
                "adl_gen2_backup_destination_folder='/<target directory in adl_gen2_filesystem to backup to>'\n",
                "\n",
                "print(f'INFO: Will backup HDFS data from {bdc_hdfs_source_directory} to abfss://{adl_gen2_filesystem}@{adl_gen2_storage_account_name}.dfs.core.windows.net{adl_gen2_backup_destination_folder}')\n",
                "\n",
                "# 4. Domain Account Name\n",
                "# domain_account_name=\"user-1234\"\n",
                "#\n",
                "domain_account_name=\"<insert domain account name>\"\n",
                "\n",
                "# 5. Domain Account Password\n",
                "domain_account_password=\"<insert domain account password>\"\n",
                ""
            ],
            "metadata": {
                "tags": [
                    "parameters"
                ],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "1cb5200f-153c-4226-9287-c6dc70a4ad87"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Common functions\n",
                "\n",
                "Define helper functions used in this notebook."
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "dc918c2f-9508-4445-a1e4-889c5887aea2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Define `run` function for transient fault handling, suggestions on error, and scrolling updates on Windows\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "import platform\n",
                "import shlex\n",
                "import shutil\n",
                "import datetime\n",
                "\n",
                "from subprocess import Popen, PIPE\n",
                "from IPython.display import Markdown\n",
                "\n",
                "retry_hints = {} # Output in stderr known to be transient, therefore automatically retry\n",
                "error_hints = {} # Output in stderr where a known SOP/TSG exists which will be HINTed for further help\n",
                "install_hint = {} # The SOP to help install the executable if it cannot be found\n",
                "\n",
                "def run(cmd, return_output=False, no_output=False, retry_count=0, base64_decode=False, return_as_json=False, regex_mask=None):\n",
                "    \"\"\"Run shell command, stream stdout, print stderr and optionally return output\n",
                "\n",
                "    NOTES:\n",
                "\n",
                "    1.  Commands that need this kind of ' quoting on Windows e.g.:\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='data-pool')].metadata.name}\n",
                "\n",
                "        Need to actually pass in as '\"':\n",
                "\n",
                "            kubectl get nodes -o jsonpath={.items[?(@.metadata.annotations.pv-candidate=='\"'data-pool'\"')].metadata.name}\n",
                "\n",
                "        The ' quote approach, although correct when pasting into Windows cmd, will hang at the line:\n",
                "        \n",
                "            `iter(p.stdout.readline, b'')`\n",
                "\n",
                "        The shlex.split call does the right thing for each platform, just use the '\"' pattern for a '\n",
                "    \"\"\"\n",
                "    MAX_RETRIES = 5\n",
                "    output = \"\"\n",
                "    retry = False\n",
                "\n",
                "    # When running `azdata sql query` on Windows, replace any \\n in \"\"\" strings, with \" \", otherwise we see:\n",
                "    #\n",
                "    #    ('HY090', '[HY090] [Microsoft][ODBC Driver Manager] Invalid string or buffer length (0) (SQLExecDirectW)')\n",
                "    #\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"azdata sql query\"):\n",
                "        cmd = cmd.replace(\"\\n\", \" \")\n",
                "\n",
                "    # shlex.split is required on bash and for Windows paths with spaces\n",
                "    #\n",
                "    cmd_actual = shlex.split(cmd)\n",
                "\n",
                "    # Store this (i.e. kubectl, python etc.) to support binary context aware error_hints and retries\n",
                "    #\n",
                "    user_provided_exe_name = cmd_actual[0].lower()\n",
                "\n",
                "    # When running python, use the python in the ADS sandbox ({sys.executable})\n",
                "    #\n",
                "    if cmd.startswith(\"python \"):\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"python\", sys.executable)\n",
                "\n",
                "        # On Mac, when ADS is not launched from terminal, LC_ALL may not be set, which causes pip installs to fail\n",
                "        # with:\n",
                "        #\n",
                "        #    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4969: ordinal not in range(128)\n",
                "        #\n",
                "        # Setting it to a default value of \"en_US.UTF-8\" enables pip install to complete\n",
                "        #\n",
                "        if platform.system() == \"Darwin\" and \"LC_ALL\" not in os.environ:\n",
                "            os.environ[\"LC_ALL\"] = \"en_US.UTF-8\"\n",
                "\n",
                "    # When running `kubectl`, if AZDATA_OPENSHIFT is set, use `oc`\n",
                "    #\n",
                "    if cmd.startswith(\"kubectl \") and \"AZDATA_OPENSHIFT\" in os.environ:\n",
                "        cmd_actual[0] = cmd_actual[0].replace(\"kubectl\", \"oc\")\n",
                "\n",
                "    # To aid supportability, determine which binary file will actually be executed on the machine\n",
                "    #\n",
                "    which_binary = None\n",
                "\n",
                "    # Special case for CURL on Windows.  The version of CURL in Windows System32 does not work to\n",
                "    # get JWT tokens, it returns \"(56) Failure when receiving data from the peer\".  If another instance\n",
                "    # of CURL exists on the machine use that one.  (Unfortunately the curl.exe in System32 is almost\n",
                "    # always the first curl.exe in the path, and it can't be uninstalled from System32, so here we\n",
                "    # look for the 2nd installation of CURL in the path)\n",
                "    if platform.system() == \"Windows\" and cmd.startswith(\"curl \"):\n",
                "        path = os.getenv('PATH')\n",
                "        for p in path.split(os.path.pathsep):\n",
                "            p = os.path.join(p, \"curl.exe\")\n",
                "            if os.path.exists(p) and os.access(p, os.X_OK):\n",
                "                if p.lower().find(\"system32\") == -1:\n",
                "                    cmd_actual[0] = p\n",
                "                    which_binary = p\n",
                "                    break\n",
                "\n",
                "    # Find the path based location (shutil.which) of the executable that will be run (and display it to aid supportability), this\n",
                "    # seems to be required for .msi installs of azdata.cmd/az.cmd.  (otherwise Popen returns FileNotFound) \n",
                "    #\n",
                "    # NOTE: Bash needs cmd to be the list of the space separated values hence shlex.split.\n",
                "    #\n",
                "    if which_binary == None:\n",
                "        which_binary = shutil.which(cmd_actual[0])\n",
                "\n",
                "    # Display an install HINT, so the user can click on a SOP to install the missing binary\n",
                "    #\n",
                "    if which_binary == None:\n",
                "        print(f\"The path used to search for '{cmd_actual[0]}' was:\")\n",
                "        print(sys.path)\n",
                "\n",
                "        if user_provided_exe_name in install_hint and install_hint[user_provided_exe_name] is not None:\n",
                "            display(Markdown(f'HINT: Use [{install_hint[user_provided_exe_name][0]}]({install_hint[user_provided_exe_name][1]}) to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\")\n",
                "    else:   \n",
                "        cmd_actual[0] = which_binary\n",
                "\n",
                "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
                "\n",
                "    cmd_display = cmd\n",
                "    if regex_mask is not None:\n",
                "        regex = re.compile(regex_mask)\n",
                "        cmd_display = re.sub(regex, '******', cmd)\n",
                "        \n",
                "    print(f\"START: {cmd_display} @ {start_time} ({datetime.datetime.utcnow().replace(microsecond=0)} UTC)\")\n",
                "    print(f\"       using: {which_binary} ({platform.system()} {platform.release()} on {platform.machine()})\")\n",
                "    print(f\"       cwd: {os.getcwd()}\")\n",
                "\n",
                "    # Command-line tools such as CURL and AZDATA HDFS commands output\n",
                "    # scrolling progress bars, which causes Jupyter to hang forever, to\n",
                "    # workaround this, use no_output=True\n",
                "    #\n",
                "\n",
                "    # Work around a infinite hang when a notebook generates a non-zero return code, break out, and do not wait\n",
                "    #\n",
                "    wait = True \n",
                "\n",
                "    try:\n",
                "        if no_output:\n",
                "            p = Popen(cmd_actual)\n",
                "        else:\n",
                "            p = Popen(cmd_actual, stdout=PIPE, stderr=PIPE, bufsize=1)\n",
                "            with p.stdout:\n",
                "                for line in iter(p.stdout.readline, b''):\n",
                "                    line = line.decode()\n",
                "                    if return_output:\n",
                "                        output = output + line\n",
                "                    else:\n",
                "                        if cmd.startswith(\"azdata notebook run\"): # Hyperlink the .ipynb file\n",
                "                            regex = re.compile('  \"(.*)\"\\: \"(.*)\"') \n",
                "                            match = regex.match(line)\n",
                "                            if match:\n",
                "                                if match.group(1).find(\"HTML\") != -1:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"{match.group(2)}\"'))\n",
                "                                else:\n",
                "                                    display(Markdown(f' - \"{match.group(1)}\": \"[{match.group(2)}]({match.group(2)})\"'))\n",
                "\n",
                "                                    wait = False\n",
                "                                    break # otherwise infinite hang, have not worked out why yet.\n",
                "                        else:\n",
                "                            print(line, end='')\n",
                "\n",
                "        if wait:\n",
                "            p.wait()\n",
                "    except FileNotFoundError as e:\n",
                "        if install_hint is not None:\n",
                "            display(Markdown(f'HINT: Use {install_hint} to resolve this issue.'))\n",
                "\n",
                "        raise FileNotFoundError(f\"Executable '{cmd_actual[0]}' not found in path (where/which)\") from e\n",
                "\n",
                "    exit_code_workaround = 0 # WORKAROUND: azdata hangs on exception from notebook on p.wait()\n",
                "\n",
                "    if not no_output:\n",
                "        for line in iter(p.stderr.readline, b''):\n",
                "            try:\n",
                "                line_decoded = line.decode()\n",
                "            except UnicodeDecodeError:\n",
                "                # NOTE: Sometimes we get characters back that cannot be decoded(), e.g.\n",
                "                #\n",
                "                #   \\xa0\n",
                "                #\n",
                "                # For example see this in the response from `az group create`:\n",
                "                #\n",
                "                # ERROR: Get Token request returned http error: 400 and server \n",
                "                # response: {\"error\":\"invalid_grant\",# \"error_description\":\"AADSTS700082: \n",
                "                # The refresh token has expired due to inactivity.\\xa0The token was \n",
                "                # issued on 2018-10-25T23:35:11.9832872Z\n",
                "                #\n",
                "                # which generates the exception:\n",
                "                #\n",
                "                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 179: invalid start byte\n",
                "                #\n",
                "                print(\"WARNING: Unable to decode stderr line, printing raw bytes:\")\n",
                "                print(line)\n",
                "                line_decoded = \"\"\n",
                "                pass\n",
                "            else:\n",
                "\n",
                "                # azdata emits a single empty line to stderr when doing an hdfs cp, don't\n",
                "                # print this empty \"ERR:\" as it confuses.\n",
                "                #\n",
                "                if line_decoded == \"\":\n",
                "                    continue\n",
                "                \n",
                "                print(f\"STDERR: {line_decoded}\", end='')\n",
                "\n",
                "                if line_decoded.startswith(\"An exception has occurred\") or line_decoded.startswith(\"ERROR: An error occurred while executing the following cell\"):\n",
                "                    exit_code_workaround = 1\n",
                "\n",
                "                # inject HINTs to next TSG/SOP based on output in stderr\n",
                "                #\n",
                "                if user_provided_exe_name in error_hints:\n",
                "                    for error_hint in error_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(error_hint[0]) != -1:\n",
                "                            display(Markdown(f'HINT: Use [{error_hint[1]}]({error_hint[2]}) to resolve this issue.'))\n",
                "\n",
                "                # Verify if a transient error, if so automatically retry (recursive)\n",
                "                #\n",
                "                if user_provided_exe_name in retry_hints:\n",
                "                    for retry_hint in retry_hints[user_provided_exe_name]:\n",
                "                        if line_decoded.find(retry_hint) != -1:\n",
                "                            if retry_count < MAX_RETRIES:\n",
                "                                print(f\"RETRY: {retry_count} (due to: {retry_hint})\")\n",
                "                                retry_count = retry_count + 1\n",
                "                                output = run(cmd, return_output=return_output, retry_count=retry_count)\n",
                "\n",
                "                                if return_output:\n",
                "                                    if base64_decode:\n",
                "                                        import base64\n",
                "                                        return base64.b64decode(output).decode('utf-8')\n",
                "                                    else:\n",
                "                                        return output\n",
                "\n",
                "    elapsed = datetime.datetime.now().replace(microsecond=0) - start_time\n",
                "\n",
                "    # WORKAROUND: We avoid infinite hang above in the `azdata notebook run` failure case, by inferring success (from stdout output), so\n",
                "    # don't wait here, if success known above\n",
                "    #\n",
                "    if wait: \n",
                "        if p.returncode != 0:\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd_display} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(p.returncode)}.\\n')\n",
                "    else:\n",
                "        if exit_code_workaround !=0 :\n",
                "            raise SystemExit(f'Shell command:\\n\\n\\t{cmd_display} ({elapsed}s elapsed)\\n\\nreturned non-zero exit code: {str(exit_code_workaround)}.\\n')\n",
                "\n",
                "    print(f'\\nSUCCESS: {elapsed}s elapsed.\\n')\n",
                "\n",
                "    if return_output:\n",
                "        if base64_decode:\n",
                "            import base64\n",
                "            return base64.b64decode(output).decode('utf-8')\n",
                "        else:\n",
                "            return output\n",
                "\n",
                "\n",
                "\n",
                "# Hints for tool retry (on transient fault), known errors and install guide\n",
                "#\n",
                "retry_hints = {'azdata': ['Endpoint sql-server-master does not exist', 'Endpoint livy does not exist', 'Failed to get state for cluster', 'Endpoint webhdfs does not exist', 'Adaptive Server is unavailable or does not exist', 'Error: Address already in use', 'Login timeout expired (0) (SQLDriverConnect)', 'SSPI Provider: No Kerberos credentials available',  ], 'kubectl': ['A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',  ], 'python': [ ], }\n",
                "error_hints = {'azdata': [['Please run \\'azdata login\\' to first authenticate', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['The token is expired', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Reason: Unauthorized', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Max retries exceeded with url: /api/v1/bdc/endpoints', 'SOP028 - azdata login', '../common/sop028-azdata-login.ipynb'], ['Look at the controller logs for more details', 'TSG027 - Observe cluster deployment', '../diagnose/tsg027-observe-bdc-create.ipynb'], ['provided port is already allocated', 'TSG062 - Get tail of all previous container logs for pods in BDC namespace', '../log-files/tsg062-tail-bdc-previous-container-logs.ipynb'], ['Create cluster failed since the existing namespace', 'SOP061 - Delete a big data cluster', '../install/sop061-delete-bdc.ipynb'], ['Failed to complete kube config setup', 'TSG067 - Failed to complete kube config setup', '../repair/tsg067-failed-to-complete-kube-config-setup.ipynb'], ['Data source name not found and no default driver specified', 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], ['Can\\'t open lib \\'ODBC Driver 17 for SQL Server', 'SOP069 - Install ODBC for SQL Server', '../install/sop069-install-odbc-driver-for-sql-server.ipynb'], ['Control plane upgrade failed. Failed to upgrade controller.', 'TSG108 - View the controller upgrade config map', '../diagnose/tsg108-controller-failed-to-upgrade.ipynb'], ['NameError: name \\'azdata_login_secret_name\\' is not defined', 'SOP013 - Create secret for azdata login (inside cluster)', '../common/sop013-create-secret-for-azdata-login.ipynb'], ['ERROR: No credentials were supplied, or the credentials were unavailable or inaccessible.', 'TSG124 - \\'No credentials were supplied\\' error from azdata login', '../repair/tsg124-no-credentials-were-supplied.ipynb'], ['Please accept the license terms to use this product through', 'TSG126 - azdata fails with \\'accept the license terms to use this product\\'', '../repair/tsg126-accept-license-terms.ipynb'],  ], 'kubectl': [['no such host', 'TSG010 - Get configuration contexts', '../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb'], ['No connection could be made because the target machine actively refused it', 'TSG056 - Kubectl fails with No connection could be made because the target machine actively refused it', '../repair/tsg056-kubectl-no-connection-could-be-made.ipynb'],  ], 'python': [['Library not loaded: /usr/local/opt/unixodbc', 'SOP012 - Install unixodbc for Mac', '../install/sop012-brew-install-odbc-for-sql-server.ipynb'], ['WARNING: You are using pip version', 'SOP040 - Upgrade pip in ADS Python sandbox', '../install/sop040-upgrade-pip.ipynb'],  ], }\n",
                "install_hint = {'azdata': [ 'SOP063 - Install azdata CLI (using package manager)', '../install/sop063-packman-install-azdata.ipynb' ],  'kubectl': [ 'SOP036 - Install kubectl command line interface', '../install/sop036-install-kubectl.ipynb' ],  }\n",
                "\n",
                "\n",
                "print('Common functions defined successfully.')"
            ],
            "metadata": {
                "tags": [
                    "hide_input"
                ],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "66386243-1575-4b27-a22f-6d2f31e26335"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Azure subscription operations"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0920786c-5e44-487c-a817-7fecb62861ad"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# If supplied as a parameter, set the azure_subscrition_id as the current active subscription.  Otherwise we'll just use default subscription for the logged in user.\n",
                "if azure_subscription_id != \"\":\n",
                "  run(f'az account set --subscription {azure_subscription_id}')\n",
                "\n",
                "# Retrieve and report current subscription ID as this is where the target storage account must reside\n",
                "current_subscription_id = run('az account list --query \"[?isDefault].id\" --output tsv', return_output=True)\n",
                "print(f'INFO: Subscription: {current_subscription_id.strip()} is the current active subscription\\n')\n",
                "\n",
                "# Retrieve the storage key for the target storage account\n",
                "adl_gen2_storage_key = run(f'az storage account keys list --account-name {adl_gen2_storage_account_name} --query [0].value --output tsv', return_output=True)"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "34d94943-402e-49ee-9aa2-5647c60babc6"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Instantiate Kubernetes client"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "d6e87249-c0c4-4da7-b3c6-3632415d3efe"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Instantiate the Python Kubernetes client into 'api' variable\n",
                "\n",
                "import os\n",
                "from IPython.display import Markdown\n",
                "\n",
                "try:\n",
                "    from kubernetes import client, config\n",
                "    from kubernetes.stream import stream\n",
                "except ImportError: \n",
                "\n",
                "    # Install the Kubernetes module\n",
                "    import sys\n",
                "    !{sys.executable} -m pip install kubernetes    \n",
                "    \n",
                "    try:\n",
                "        from kubernetes import client, config\n",
                "        from kubernetes.stream import stream\n",
                "    except ImportError:\n",
                "        display(Markdown(f'HINT: Use [SOP059 - Install Kubernetes Python module](../install/sop059-install-kubernetes-module.ipynb) to resolve this issue.'))\n",
                "        raise\n",
                "\n",
                "if \"KUBERNETES_SERVICE_PORT\" in os.environ and \"KUBERNETES_SERVICE_HOST\" in os.environ:\n",
                "    config.load_incluster_config()\n",
                "else:\n",
                "    try:\n",
                "        config.load_kube_config()\n",
                "    except:\n",
                "        display(Markdown(f'HINT: Use [TSG118 - Configure Kubernetes config](../repair/tsg118-configure-kube-config.ipynb) to resolve this issue.'))\n",
                "        raise\n",
                "\n",
                "api = client.CoreV1Api()\n",
                "\n",
                "print('Kubernetes client instantiated')"
            ],
            "metadata": {
                "tags": [
                    "hide_input"
                ],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a4c626c9-9cdf-4d7c-b196-07196281e6b3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Get the namespace for the big data cluster\n",
                "\n",
                "Get the namespace of the Big Data Cluster from the Kuberenetes API.\n",
                "\n",
                "**NOTE:**\n",
                "\n",
                "If there is more than one Big Data Cluster in the target Kubernetes\n",
                "cluster, then either:\n",
                "\n",
                "-   set \\[0\\] to the correct value for the big data cluster.\n",
                "-   set the environment variable AZDATA\\_NAMESPACE, before starting\n",
                "    Azure Data Studio."
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "781c96a9-9f94-4ad2-a1bf-26497ab3f701"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Place Kubernetes namespace name for BDC into 'namespace' variable\n",
                "\n",
                "if \"AZDATA_NAMESPACE\" in os.environ:\n",
                "    namespace = os.environ[\"AZDATA_NAMESPACE\"]\n",
                "else:\n",
                "    try:\n",
                "        namespace = api.list_namespace(label_selector='MSSQL_CLUSTER').items[0].metadata.name\n",
                "    except IndexError:\n",
                "        from IPython.display import Markdown\n",
                "        display(Markdown(f'HINT: Use [TSG081 - Get namespaces (Kubernetes)](../monitor-k8s/tsg081-get-kubernetes-namespaces.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [TSG010 - Get configuration contexts](../monitor-k8s/tsg010-get-kubernetes-contexts.ipynb) to resolve this issue.'))\n",
                "        display(Markdown(f'HINT: Use [SOP011 - Set kubernetes configuration context](../common/sop011-set-kubernetes-context.ipynb) to resolve this issue.'))\n",
                "        raise\n",
                "\n",
                "print('The kubernetes namespace for your big data cluster is: ' + namespace)"
            ],
            "metadata": {
                "tags": [
                    "hide_input"
                ],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a5dd3fe4-f702-4c02-b232-bffdb6f9b7d9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Get the name of the namenode pod"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0387e8a7-7a23-4c5f-87a0-b58fb4ae759d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "namenode_pod = run(f'kubectl get pod --selector=role=namenode -n {namespace} -o jsonpath={{.items[0].metadata.name}}', return_output=True)\n",
                "\n",
                "print (f'INFO: Namenode pod name: {namenode_pod}')"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b16ac18e-b621-4149-af27-85b043815326"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Run distcp command\n",
                "\n",
                "The distcp command supports many different options. The command below is\n",
                "just a sample, and should be modified to align with your unique\n",
                "requirements. [You can read more about distcp\n",
                "here](https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html)."
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a2ea8a3d-5e4f-48f1-bad8-921750ff2389"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from enum import Enum\n",
                "class CopyStrategy(Enum):\n",
                "  DYNAMIC = \"dynamic\"\n",
                "  UNIFORMSIZE = \"uniformsize\"\n",
                "\n",
                "# Set account key name to utilize on distcp command.  This key in combination with the associated value authorizes the write to the tareget account\n",
                "fs_azure_account_key = f'fs.azure.account.key.{adl_gen2_storage_account_name}.dfs.core.windows.net'\n",
                "\n",
                "# Target URI of Gen2 storage account which will be destination of the backup operation\n",
                "target_storage_uri = f'abfss://{adl_gen2_filesystem}@{adl_gen2_storage_account_name}.dfs.core.windows.net{adl_gen2_backup_destination_folder}'\n",
                "\n",
                "# Location of cloud storage drivers\n",
                "libjars_path = '/opt/hadoop/share/hadoop/tools/lib/\\*'\n",
                "\n",
                "name=namenode_pod\n",
                "container='hadoop'\n",
                "\n",
                "# User kinit\n",
                "kinit_command = f'kinit {domain_account_name} <<< {domain_account_password}'\n",
                "\n",
                "kinit_string=stream(api.connect_get_namespaced_pod_exec, name, namespace, command=['/bin/bash', '-c', kinit_command], container=container, stderr=True, stdout=True)\n",
                "\n",
                "# The distcp command has many options.  This is just a sample command.  \n",
                "command = f'hadoop distcp -D {fs_azure_account_key}={adl_gen2_storage_key.strip()} -libjars {libjars_path} -strategy {CopyStrategy.DYNAMIC.value} {bdc_hdfs_source_directory} {target_storage_uri}'\n",
                "\n",
                "# Run the command\n",
                "string=stream(api.connect_get_namespaced_pod_exec, name, namespace, command=['/bin/bash', '-c', command], container=container, stderr=True, stdout=True)\n",
                "\n",
                "print(string)"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "93ecf6c4-c189-4464-8adc-9a6b63f076b0"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"Notebook execution is complete.\")"
            ],
            "metadata": {
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0da6964a-274f-4356-9e89-0bfac6fe6333"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}